{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#install"
      ],
      "metadata": {
        "id": "wzNTshx8xAGg"
      },
      "id": "wzNTshx8xAGg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monai\n",
        "!pip install nibabel\n",
        "!pip install imageio\n",
        "!pip install natsort\n",
        "!pip install lpips\n",
        "!pip install kornia"
      ],
      "metadata": {
        "id": "evlz0nJwALe-"
      },
      "id": "evlz0nJwALe-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a06dabd4-1a7f-4aeb-a873-18a404c9d053",
      "metadata": {
        "id": "a06dabd4-1a7f-4aeb-a873-18a404c9d053"
      },
      "source": [
        "# One sided paired Medical Image Translation with Normalized Edge Priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf47062-9991-4dbc-bd29-55bd2e672931",
      "metadata": {
        "id": "ddf47062-9991-4dbc-bd29-55bd2e672931"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os, glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from monai.config import print_config\n",
        "from monai.data import DataLoader, Dataset, CacheDataset, PatchDataset, SmartCacheDataset\n",
        "from monai.inferers import SliceInferer,sliding_window_inference\n",
        "from monai.utils import set_determinism, first\n",
        "from monai.transforms import(\n",
        "    Compose,\n",
        "    Lambdad,\n",
        "    LoadImaged,\n",
        "    SaveImage,\n",
        "    EnsureChannelFirstd,\n",
        "    SqueezeDimd,\n",
        "    RandSpatialCropSamplesd,\n",
        "    ScaleIntensityRangePercentilesd,\n",
        "    ScaleIntensityRanged,\n",
        "    EnsureTyped,\n",
        "    Resized,\n",
        "    CropForegroundd,\n",
        "    CenterSpatialCropd,\n",
        "    RandZoomd\n",
        ")\n",
        "# print_config()\n",
        "from datetime import date\n",
        "today = str(date.today()).replace('-','').replace(' ', '')\n",
        "gpu_device = torch.device(f'cuda:{0}')\n",
        "weights_output_dir = 'Weights'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa71755-7a7a-49e2-b04c-69a1b75bad52",
      "metadata": {
        "id": "7fa71755-7a7a-49e2-b04c-69a1b75bad52"
      },
      "source": [
        "Get gamma images, define splits, slice them and write into a 2D folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339dcd27-41b2-4901-8f75-ca6b4fec53e3",
      "metadata": {
        "id": "339dcd27-41b2-4901-8f75-ca6b4fec53e3"
      },
      "outputs": [],
      "source": [
        "suffix2d = '_2d_transformed'\n",
        "MRs='MR'\n",
        "CTs='CT'\n",
        "MASKs='MASK'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d398f7c8-036b-455b-9da9-f10a955870c9",
      "metadata": {
        "id": "d398f7c8-036b-455b-9da9-f10a955870c9"
      },
      "source": [
        "# 2D processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76a5777-b008-442a-8d0c-74a331f9535d",
      "metadata": {
        "id": "c76a5777-b008-442a-8d0c-74a331f9535d"
      },
      "outputs": [],
      "source": [
        "transforms_2d = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"SRC\", \"TGT\", \"MASK\"], image_only=False),\n",
        "        EnsureChannelFirstd(keys=[\"SRC\", \"TGT\", \"MASK\"]),\n",
        "        EnsureTyped(keys=[\"SRC\", \"TGT\", \"MASK\"], dtype=torch.float32),\n",
        "    ]\n",
        ")\n",
        "\n",
        "BATCH_SIZE=14\n",
        "NUM_WORKERS=12\n",
        "\n",
        "MRs_sufx = os.path.join(MRs + suffix2d)\n",
        "CTs_sufx = os.path.join(CTs + suffix2d)\n",
        "MASKs_sufx = os.path.join(MASKs + suffix2d)\n",
        "\n",
        "# transforms_2d = Compose(\n",
        "#     [\n",
        "#         LoadImaged(keys=[\"SRC\", \"TGT\"], image_only=False),\n",
        "#         EnsureChannelFirstd(keys=[\"SRC\", \"TGT\"]),\n",
        "#         EnsureTyped(keys=[\"SRC\", \"TGT\"], dtype=torch.float32),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# BATCH_SIZE=14\n",
        "# NUM_WORKERS=12\n",
        "\n",
        "# MRs_sufx = os.path.join(MRs + suffix2d)\n",
        "# CTs_sufx = os.path.join(CTs + suffix2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9d2655-de9e-4134-a77b-c9be5bc044b0",
      "metadata": {
        "id": "fa9d2655-de9e-4134-a77b-c9be5bc044b0"
      },
      "source": [
        "- Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451f482f-4d5c-46a5-aaa1-e32f320eb163",
      "metadata": {
        "id": "451f482f-4d5c-46a5-aaa1-e32f320eb163"
      },
      "outputs": [],
      "source": [
        "fnames_train_A_2d = sorted(glob.glob(os.path.join(MRs_sufx, 'train_mr', '*.nii.gz')))\n",
        "fnames_train_B_2d = sorted(glob.glob(os.path.join(CTs_sufx, 'train_ct', '*.nii.gz')))\n",
        "fnames_train_C_2d = sorted(glob.glob(os.path.join(MASKs_sufx, 'train_mask', '*.nii.gz')))\n",
        "train_dic_2d = [{\"SRC\": img1, \"TGT\": img2, \"MASK\": img3} for (img1,img2,img3) in zip(\n",
        "    fnames_train_A_2d,\n",
        "    fnames_train_B_2d,\n",
        "    fnames_train_C_2d\n",
        ")]\n",
        "\n",
        "train_ds = CacheDataset(train_dic_2d, transforms_2d)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "# fnames_train_A_2d = sorted(glob.glob(os.path.join(MRs_sufx, 'train_mr', '*.nii.gz')))\n",
        "# fnames_train_B_2d = sorted(glob.glob(os.path.join(CTs_sufx, 'train_ct', '*.nii.gz')))\n",
        "# train_dic_2d = [{\"SRC\": img1, \"TGT\": img2} for (img1,img2) in zip(\n",
        "#     fnames_train_A_2d,\n",
        "#     fnames_train_B_2d\n",
        "# )]\n",
        "\n",
        "# train_ds = CacheDataset(train_dic_2d, transforms_2d)\n",
        "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044a0c7b-baf1-48b3-bc24-49d6c907cac9",
      "metadata": {
        "id": "044a0c7b-baf1-48b3-bc24-49d6c907cac9"
      },
      "source": [
        "- Val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3195a8-9fc3-4fc5-9268-60d7f025ded3",
      "metadata": {
        "tags": [],
        "id": "0b3195a8-9fc3-4fc5-9268-60d7f025ded3"
      },
      "outputs": [],
      "source": [
        "fnames_val_A_2d = sorted(glob.glob(os.path.join(MRs_sufx, 'val_mr', '*.nii.gz')))\n",
        "fnames_val_B_2d = sorted(glob.glob(os.path.join(CTs_sufx, 'val_ct', '*.nii.gz')))\n",
        "fnames_val_C_2d = sorted(glob.glob(os.path.join(MASKs_sufx, 'val_mask', '*.nii.gz')))\n",
        "val_dic_2d = [{\"SRC\": img1, \"TGT\": img2, \"MASK\": img3} for (img1,img2,img3) in zip(\n",
        "    fnames_val_A_2d,\n",
        "    fnames_val_B_2d,\n",
        "    fnames_val_C_2d\n",
        ")]\n",
        "\n",
        "val_ds = CacheDataset(val_dic_2d, transforms_2d)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "# fnames_val_A_2d = sorted(glob.glob(os.path.join(MRs_sufx, 'val_mr', '*.nii.gz')))\n",
        "# fnames_val_B_2d = sorted(glob.glob(os.path.join(CTs_sufx, 'val_ct', '*.nii.gz')))\n",
        "# val_dic_2d = [{\"SRC\": img1, \"TGT\": img2} for (img1,img2) in zip(\n",
        "#     fnames_val_A_2d,\n",
        "#     fnames_val_B_2d\n",
        "# )]\n",
        "\n",
        "# val_ds = CacheDataset(val_dic_2d, transforms_2d)\n",
        "# val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a6435c-fcbc-4666-87e0-63fdcb1648d7",
      "metadata": {
        "id": "40a6435c-fcbc-4666-87e0-63fdcb1648d7"
      },
      "outputs": [],
      "source": [
        "check_data = first(train_loader)\n",
        "print(\"first patch's shape: \", check_data[\"SRC\"].shape)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,4,1)\n",
        "plt.imshow(check_data[\"TGT\"][0,0,:,:].detach().cpu().numpy().squeeze(), vmin=-1, vmax=1, cmap=\"gray\")\n",
        "plt.title('Target')\n",
        "plt.subplot(1,4,2)\n",
        "plt.imshow(check_data[\"SRC\"][0,0,:,:].detach().cpu().numpy().squeeze(), vmin=-1, vmax=1, cmap=\"gray\")\n",
        "plt.title('Source')\n",
        "plt.subplot(1,4,3)\n",
        "plt.imshow(check_data[\"MASK\"][0,0,:,:].detach().cpu().numpy().squeeze(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "plt.title('Mask')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7540280b",
      "metadata": {
        "id": "7540280b"
      },
      "source": [
        "# Network training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91eee6ec-93e2-4943-9d26-29d52c9f3306",
      "metadata": {
        "id": "91eee6ec-93e2-4943-9d26-29d52c9f3306"
      },
      "outputs": [],
      "source": [
        "from vjnetworks import Pix2Pix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188a78f6-a65a-4ab4-9979-eaddb1412337",
      "metadata": {
        "id": "188a78f6-a65a-4ab4-9979-eaddb1412337"
      },
      "outputs": [],
      "source": [
        "class Options():\n",
        "    def __init__(self):\n",
        "        # model parameters\n",
        "        self.in_channels = 1  # Adjust according to your input image channel dimensions\n",
        "        self.out_channels = 1  # Adjust according to your output image channel dimensions\n",
        "        self.num_filters_d = 128  # Adjust the number of filters in the discriminator\n",
        "        self.num_layers_d = 4  # Adjust the number of layers in the discriminator (i.e. the receptive field)\n",
        "        self.num_d = 2\n",
        "        self.num_res_units_G = 10\n",
        "        self.lambda_gan = 1 # Adjust the weight for the cycle consistency loss\n",
        "\n",
        "        # training parameters\n",
        "        self.num_epochs=200  # 300 is enough\n",
        "        self.learning_rate = 2e-4  # typical value for CycleGAN\n",
        "        self.lambda_identity = 0  # Adjust the weight for the identity loss\n",
        "        self.lambda_bg = 2.0 # bg-air term used when working with weighted masks\n",
        "        self.lambda_NGF=20 # best 20\n",
        "        self.alpha_NGF=.08 # best .15\n",
        "        self.lambda_l1=140 #  # best 100 for pix2pix\n",
        "\n",
        "opt=Options()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851976a2-db22-4213-b76b-453c77513d2b",
      "metadata": {
        "id": "851976a2-db22-4213-b76b-453c77513d2b"
      },
      "outputs": [],
      "source": [
        "import torch.optim\n",
        "import torch.nn.functional as F\n",
        "import monai.networks.nets as nets\n",
        "\n",
        "pix2pix_model = Pix2Pix(\n",
        "    in_channels=opt.in_channels,\n",
        "    out_channels=opt.out_channels,\n",
        "    num_d=opt.num_d,\n",
        "    num_layers_d=opt.num_layers_d,\n",
        "    num_filters_d=opt.num_filters_d,\n",
        "    num_res_units_G=opt.num_res_units_G,\n",
        ")\n",
        "air = -1.0 # usually min value of normalizer\n",
        "optimizer = torch.optim.Adam(pix2pix_model.parameters(), lr=opt.learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba78ec",
      "metadata": {
        "id": "6aba78ec"
      },
      "source": [
        "#### load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6641e809-7320-4fc0-a6d9-e54c8deb1751",
      "metadata": {
        "id": "6641e809-7320-4fc0-a6d9-e54c8deb1751",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "weights_path = 'Weights/Gamma_crop-n2-l4-f128_GAN1_L140.00_NGF20.00_a0.08_e0074.h5'\n",
        "checkpoint = torch.load(weights_path)\n",
        "pix2pix_model.load_state_dict(checkpoint['model'], strict=False)\n",
        "pix2pix_model.to(gpu_device).eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca08141f",
      "metadata": {
        "id": "ca08141f"
      },
      "source": [
        "#### suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ad9eb4-0916-416d-b351-6b622aaa7535",
      "metadata": {
        "id": "b7ad9eb4-0916-416d-b351-6b622aaa7535"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_PREFIX='Gamma_crop-n%d-l%d-f%d' % (opt.num_d, opt.num_layers_d, opt.num_filters_d) + '_GAN%d_L%.2f_NGF%.2f_a%.2f' % (opt.lambda_gan, opt.lambda_l1, opt.lambda_NGF, opt.alpha_NGF)\n",
        "\n",
        "weights_dir=os.path.join(weights_output_dir, EXPERIMENT_PREFIX)\n",
        "os.makedirs(weights_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c6a620-d67f-4c87-91b1-9b517ad55640",
      "metadata": {
        "id": "c3c6a620-d67f-4c87-91b1-9b517ad55640"
      },
      "outputs": [],
      "source": [
        "import imageio.v2 as imageio\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import mean_squared_error as mse\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.exposure import match_histograms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_file = 'logs-n%d-l%d-f%d' % (opt.num_d, opt.num_layers_d, opt.num_filters_d) + '_GAN%d_L%.2f_NGF%.2f_a%.2f' % (opt.lambda_gan, opt.lambda_l1, opt.lambda_NGF, opt.alpha_NGF) +'.csv'\n",
        "columns = ['epoch', 'psnr global','local psnr', 'loss 1', 'loss 2','G loss', 'D loss']\n",
        "if os.path.exists(os.path.join(weights_dir,csv_file)):\n",
        "  df = pd.read_csv(os.path.join(weights_dir,csv_file))\n",
        "else:\n",
        "  df = pd.DataFrame(columns=columns)\n",
        "  df.to_csv(os.path.join(weights_dir,csv_file), index=False)\n",
        "\n",
        "def update_csv(epoch, psnr,lpsnr, loss1, loss2, gloss, dloss):\n",
        "  global df\n",
        "  new_row = pd.DataFrame([[round(epoch,5),round( psnr,5),round(lpsnr,5), round(loss1,5), round(loss2,5), round(gloss,5), round(dloss,5)]], columns=columns)\n",
        "  df = pd.concat([df, new_row], ignore_index=True)\n",
        "  df.to_csv(os.path.join(weights_dir,csv_file), index=False)"
      ],
      "metadata": {
        "id": "XVnn1Tz77bCm"
      },
      "id": "XVnn1Tz77bCm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sliceinferer"
      ],
      "metadata": {
        "id": "7j2B5t7tKqx8"
      },
      "id": "7j2B5t7tKqx8"
    },
    {
      "cell_type": "code",
      "source": [
        "roi_size = ((256,256))\n",
        "sw_batch_size=BATCH_SIZE"
      ],
      "metadata": {
        "id": "X0cem_TSJwi9"
      },
      "id": "X0cem_TSJwi9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def dilate2d(M, pixels= 2):\n",
        "    M = (M > 0.5).to(torch.float32)\n",
        "    k = 2 * pixels + 1\n",
        "    Md = F.max_pool2d(M, kernel_size=k, stride=1, padding=pixels)\n",
        "    return (Md > 0.0).to(M.dtype)"
      ],
      "metadata": {
        "id": "pOGprqTjOagz"
      },
      "id": "pOGprqTjOagz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3f623869",
      "metadata": {
        "id": "3f623869"
      },
      "source": [
        "#### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6775a8-530e-433b-860a-eec5b5e1ebc0",
      "metadata": {
        "tags": [],
        "id": "ed6775a8-530e-433b-860a-eec5b5e1ebc0"
      },
      "outputs": [],
      "source": [
        "PSNRs=[]\n",
        "best_psnr=[]\n",
        "for epoch in np.arange(76,opt.num_epochs):\n",
        "# for epoch in range(opt.num_epochs):\n",
        "    loop = tqdm(train_loader)\n",
        "    loop.set_description(f\"Epoch [{epoch}/{opt.num_epochs}]\")\n",
        "\n",
        "    # this is autoencoding for the beginning\n",
        "    if epoch < 2:\n",
        "        real_B_is = \"SRC\"\n",
        "    else:\n",
        "    # this is normal unpaired image translation behaviour\n",
        "        real_B_is = \"TGT\"\n",
        "\n",
        "    # real_B_is = \"TGT\"\n",
        "    for batch_idx, real in enumerate(loop):\n",
        "        # Transfer data to the device (CPU or GPU)\n",
        "        real_A = real[\"SRC\"].to(gpu_device)\n",
        "        real_B = real[real_B_is].to(gpu_device)\n",
        "        real_C = real[\"MASK\"].to(gpu_device)\n",
        "        real_C = dilate2d(real_C, pixels=2)\n",
        "\n",
        "        # Forward pass\n",
        "        fake_B, identity_B, pred_fake_B = pix2pix_model(real_A, real_B, is_training=True)\n",
        "        generator_loss = 0.0\n",
        "\n",
        "        real_Am = real_A*real_C\n",
        "        real_Bm = real_B*real_C\n",
        "        fake_Bm = fake_B*real_C\n",
        "        identity_Bm = identity_B*real_C\n",
        "\n",
        "        # ####\n",
        "        # real_Am = real_A\n",
        "        # real_Bm = real_B\n",
        "        # fake_Bm = fake_B\n",
        "        # identity_Bm = identity_B\n",
        "        # ####\n",
        "\n",
        "        if opt.lambda_l1:\n",
        "            l1_loss = pix2pix_model.compute_l1_loss(fake_Bm, real_Bm)\n",
        "            generator_loss += opt.lambda_l1*l1_loss\n",
        "        if opt.lambda_NGF:\n",
        "            NGF_loss = pix2pix_model.compute_NGF_loss(fake_Bm, real_Am, opt.alpha_NGF)\n",
        "            generator_loss += opt.lambda_NGF*NGF_loss\n",
        "        if opt.lambda_gan:\n",
        "            pred_fake_Bm = pix2pix_model.discriminator_B(fake_Bm)\n",
        "            adv_loss = pix2pix_model.compute_adv_loss(pred_fake_Bm)\n",
        "            generator_loss += opt.lambda_gan*adv_loss\n",
        "        if opt.lambda_bg:\n",
        "            bg_loss = ((1.0 - real_C) * (fake_B - air).abs()).mean()\n",
        "            generator_loss += opt.lambda_bg*bg_loss\n",
        "        if opt.lambda_identity:\n",
        "            identity_loss = pix2pix_model.compute_l1_loss(real_Bm, identity_Bm)\n",
        "            generator_loss += opt.lambda_identity*identity_loss\n",
        "\n",
        "        # discriminator\n",
        "        discriminator_loss = pix2pix_model.compute_discriminator_loss(real_Bm, fake_Bm.detach())\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        generator_loss.backward()\n",
        "        discriminator_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if opt.lambda_NGF:\n",
        "            loop.set_postfix(losses=f\"[ {(adv_loss * opt.lambda_gan).item():.2f} {(NGF_loss * opt.lambda_NGF).item():.5f}] \")\n",
        "\n",
        "    # VALIDATION\n",
        "    model = pix2pix_model.generator_A_to_B\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      current_global_psnrs=[]\n",
        "      current_local_psnrs=[]\n",
        "      for i, real_val in enumerate(val_loader):\n",
        "          real_A_val = real_val[\"SRC\"].to(gpu_device)\n",
        "          real_B_val = real_val[\"TGT\"].to(gpu_device)\n",
        "          real_C_val = real_val[\"MASK\"].to(gpu_device)\n",
        "          real_C_val = dilate2d(real_C_val, pixels=2)\n",
        "\n",
        "          fake_B_val = sliding_window_inference(\n",
        "                        roi_size=roi_size,\n",
        "                        inputs=real_A_val,\n",
        "                        sw_batch_size=sw_batch_size,\n",
        "                        predictor=model,\n",
        "                        overlap=0.75,\n",
        "                        mode='gaussian',\n",
        "                        sigma_scale=0.5,\n",
        "                        device=gpu_device,\n",
        "                        padding_mode=\"replicate\",\n",
        "                    )\n",
        "\n",
        "\n",
        "          # saving val samples img\n",
        "          plt.figure(figsize=(15,5))\n",
        "          plt.subplot(1,3,1)\n",
        "          plt.title(\"MR\")\n",
        "          plt.imshow(real_A_val[0,0,:,:,].cpu().detach().numpy().squeeze(), vmin=-1, vmax=1, cmap='gray')\n",
        "          plt.subplot(1,3,2)\n",
        "          plt.title(\"CT\")\n",
        "          plt.imshow(real_B_val[0,0,:,:,].cpu().detach().numpy().squeeze(), vmin=-1, vmax=1, cmap='gray')\n",
        "          plt.subplot(1,3,3)\n",
        "          plt.title(\"FAKE\")\n",
        "          plt.imshow(fake_B_val[0,0,:,:,].cpu().detach().numpy().squeeze(), vmin=-1, vmax=1, cmap='gray')\n",
        "          fprefix = real_val['SRC_meta_dict']['filename_or_obj'][0].split('/')[-1].split('.')[0]\n",
        "          fname_out = os.path.join(weights_dir, (fprefix+'_result_e%.3d.png' % epoch))\n",
        "          plt.savefig(fname_out, bbox_inches='tight')\n",
        "          plt.close()\n",
        "\n",
        "          # local psnr on masked images\n",
        "          EPS = 1e-8\n",
        "          num = ((fake_B_val - real_B_val)**2 * real_C_val).flatten(1).sum(1)\n",
        "          den = real_C_val.flatten(1).sum(1).clamp_min(EPS)\n",
        "          mse = num / den\n",
        "          psnr_masked = 20*torch.log10(torch.tensor(2.0, device=gpu_device)) - 10*torch.log10(mse + EPS)\n",
        "          val_psnr_masked = float(psnr_masked.mean())\n",
        "          current_local_psnrs.append(val_psnr_masked)\n",
        "\n",
        "\n",
        "          # global psnr\n",
        "          P = fake_B_val.detach().cpu().numpy().astype(np.float32)\n",
        "          G = real_B_val.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "          current_global_psnrs.extend([psnr(G[b, 0], P[b, 0], data_range=2.0)for b in range(P.shape[0])])\n",
        "\n",
        "\n",
        "\n",
        "    PSNRs.append(np.mean(np.asarray(current_global_psnrs)))\n",
        "    best_psnr.append(PSNRs[-1])\n",
        "    print('average PSNR = %.3f' % PSNRs[-1])\n",
        "    update_csv(epoch, PSNRs[-1],np.mean(current_local_psnrs), (adv_loss * opt.lambda_gan).item(), (NGF_loss * opt.lambda_NGF).item(), generator_loss.item(), discriminator_loss.item())\n",
        "\n",
        "    if PSNRs[-1] == max(best_psnr):\n",
        "        model_weights = pix2pix_model.state_dict()\n",
        "        best_epoch = epoch\n",
        "\n",
        "    if (epoch +1) %20 ==0:\n",
        "        best_psnr = []\n",
        "        torch.save({'model': model_weights}, os.path.join(weights_dir, EXPERIMENT_PREFIX+'_e%.4d.h5' % best_epoch))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}