{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06dabd4-1a7f-4aeb-a873-18a404c9d053",
   "metadata": {},
   "source": [
    "# One sided Unpaired Medical Image Translation with Normalized Edge Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf47062-9991-4dbc-bd29-55bd2e672931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os, glob\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, SmartCacheDataset, decollate_batch\n",
    "from monai.inferers import SliceInferer , sliding_window_inference\n",
    "from monai.metrics.regression import compute_ssim_and_cs\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import(\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    SaveImage,\n",
    "    EnsureChannelFirstd,\n",
    "    SqueezeDimd, \n",
    "    EnsureTyped,\n",
    "    ResampleToMatchd,\n",
    "    RandSpatialCropSamplesd, \n",
    "    ScaleIntensityRangePercentilesd, \n",
    "    ScaleIntensityRanged, \n",
    "    Resized,\n",
    "    CropForegroundd, \n",
    "    CenterSpatialCropd, \n",
    "    RandZoomd\n",
    ")\n",
    "# print_config()\n",
    "from datetime import date\n",
    "today = str(date.today()).replace('-','').replace(' ', '')\n",
    "gpu_device = torch.device(f'cuda:{0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa71755-7a7a-49e2-b04c-69a1b75bad52",
   "metadata": {},
   "source": [
    "Get original BraTS images, define splits, slice them and write into a 2D folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dcd27-41b2-4901-8f75-ca6b4fec53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix2d = '_2d_transformed'\n",
    "import glob\n",
    "MRs='MR'\n",
    "CTs='CT'\n",
    "Masks = 'MASK'\n",
    "PIDs_ALL = [i.split('/')[-1] for i in glob.glob(os.path.join(MRs,'Gamma*'))]\n",
    "N=round(len(PIDs_ALL)*0.7)\n",
    "np.random.seed(29100)\n",
    "np.random.shuffle(PIDs_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84104b2e-1bcb-498d-96fc-9afd71b11e76",
   "metadata": {},
   "source": [
    "1) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19784f8f-a30b-4af4-90da-20103ffb8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIDs_train = PIDs_ALL[:N]\n",
    "\n",
    "# np.random.shuffle(PIDs_train_B)\n",
    "\n",
    "fnames_train_A_3d = [os.path.join(MRs,PID) for PID in PIDs_train]\n",
    "fnames_train_B_3d = [os.path.join(CTs,PID) for PID in PIDs_train]\n",
    "fnames_train_C_3d = [os.path.join(CTs+'_Seg_Mask',PID.split('.')[0]+'_mask','body_extremities.nii.gz') for PID in PIDs_train]\n",
    "train_dic_3d = [{\"SRC\": img1, \"TGT\": img2, \"MASK\": img3} for (img1,img2,img3) in zip(\n",
    "    fnames_train_A_3d, \n",
    "    fnames_train_B_3d,\n",
    "    fnames_train_C_3d\n",
    ")] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de549b9-c530-4051-8de8-d135e497cabd",
   "metadata": {},
   "source": [
    "2. Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc38d6d-d5d2-495c-86e9-5d238c806b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIDs_val = PIDs_ALL[N:N+round(len(PIDs_ALL)*0.2)]\n",
    "N = N + round(len(PIDs_ALL)*0.2)\n",
    "\n",
    "\n",
    "fnames_val_A_3d = [os.path.join(MRs,PID) for PID in PIDs_val]\n",
    "fnames_val_B_3d = [os.path.join(CTs,PID) for PID in PIDs_val]\n",
    "fnames_val_C_3d = [os.path.join(CTs+'_Seg_Mask',PID.split('.')[0]+'_mask','body_extremities.nii.gz') for PID in PIDs_val]\n",
    "val_dic_3d = [{\"SRC\": img1, \"TGT\": img2, \"MASK\":img3} for (img1,img2,img3) in zip(\n",
    "    fnames_val_A_3d, \n",
    "    fnames_val_B_3d,\n",
    "    fnames_val_C_3d\n",
    ")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f84259",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIDs_test = PIDs_ALL[N:N+round(len(PIDs_ALL)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(PIDs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd144253-1b52-4136-b4fd-62d0e89c8bdc",
   "metadata": {},
   "source": [
    "# 3D to 2D pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876c299-808e-4738-83c4-95f99a0fc180",
   "metadata": {},
   "source": [
    "#### Conversion 3D to 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b85b36-bc6d-4edd-a749-0b75cd1a67cd",
   "metadata": {},
   "source": [
    "##### Custom Rand spatial transform to reject slices that are too \"black\" (too many zeroes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bd116-9629-4e67-91b2-ad07dbafdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandSpatialCropSamplesdWithMinNonZero(RandSpatialCropSamplesd):\n",
    "    def __init__(self,target, min_nonzero: int=1000, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the custom transform.\n",
    "        :param min_nonzero: Minimum number of non-zero pixels required in the crop.\n",
    "        :param args: Arguments for RandSpatialCropSamplesd.\n",
    "        :param kwargs: Keyword arguments for RandSpatialCropSamplesd.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.min_nonzero = min_nonzero\n",
    "        if target:\n",
    "            self.target = target\n",
    "        \n",
    "\n",
    "    def __call__(self, data, lazy=False):\n",
    "        \"\"\"\n",
    "        Generate samples and filter them based on the non-zero pixel count.\n",
    "        :param data: Input data.\n",
    "        :return: Modified data with sampled crops meeting the non-zero pixel requirement.\n",
    "        \"\"\"\n",
    "        # Use the base class to generate initial samples.\n",
    "        samples = super().__call__(data)\n",
    "        valid_samples = []\n",
    "\n",
    "        # Check each sample for the non-zero pixel condition.\n",
    "        for sample in samples:\n",
    "            for key in self.keys:\n",
    "                if key == self.target:\n",
    "                    #checking if at least is there some stuff in the slice \n",
    "                    if np.sum(sample[key] > 0.4) >= self.min_nonzero:\n",
    "                        valid_samples.append(sample)\n",
    "                        break  # Assuming at least one key meets the condition is enough.\n",
    "\n",
    "        # Ensure we have at least one valid sample to avoid empty returns.\n",
    "        if not valid_samples:\n",
    "            raise ValueError(\"No valid samples found. Consider adjusting the min_nonzero parameter.\")\n",
    "\n",
    "        return valid_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1878a-f245-4b7b-a5e6-72c47eac336a",
   "metadata": {},
   "source": [
    "Apply transforms : Perform the 3D to 2D slicing and transforms, then write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3192a-2dc5-40ad-b2bd-e8276e617ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_MAX=30\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"SRC\", \"TGT\", \"MASK\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"SRC\", \"TGT\", \"MASK\"]),\n",
    "        ScaleIntensityRangePercentilesd(keys=[\"SRC\"], lower=1, upper=99.9, b_min=0,b_max=1, clip=True),\n",
    "        CenterSpatialCropd(keys=[\"SRC\", \"TGT\", \"MASK\"],roi_size=(256, 256, -1)),\n",
    "        RandSpatialCropSamplesdWithMinNonZero(keys=[\"SRC\", \"TGT\", \"MASK\"],target=\"SRC\", roi_size=(-1,-1,1), random_size=False, num_samples=NUM_SAMPLES_MAX),\n",
    "\n",
    "        ScaleIntensityRanged(keys=[\"SRC\"], a_min=0, a_max=1, b_min=-1, b_max=1),\n",
    "        ScaleIntensityRanged(keys=[\"TGT\"], a_min=-1000, a_max= 3000, b_min=-1, b_max=1),\n",
    "\n",
    "        # Resized(keys=[\"SRC\", \"TGT\"], spatial_size=[256,256,-1], mode=\"trilinear\"), # make it 256**2 to make sure we downsample correctly\n",
    "        SqueezeDimd(keys=[\"SRC\", \"TGT\", \"MASK\"], dim=0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "valtest_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"SRC\", \"TGT\", \"MASK\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"SRC\", \"TGT\", \"MASK\"]),\n",
    "\n",
    "        ScaleIntensityRangePercentilesd(keys=[\"SRC\"], lower=1, upper=99.9, b_min=0,b_max=1, clip=True),\n",
    "        # RandSpatialCropSamplesdWithMinNonZero(keys=[\"SRC\", \"TGT\"], roi_size=(-1,-1,1), random_size=False, num_samples=1),\n",
    "\n",
    "        ScaleIntensityRanged(keys=[\"SRC\"], a_min=0, a_max = 1, b_min=-1, b_max=1),\n",
    "        ScaleIntensityRanged(keys=[\"TGT\"], a_min=-1000, a_max = 3000, b_min=-1, b_max=1),\n",
    "\n",
    "        CenterSpatialCropd(keys=[\"SRC\", \"TGT\", \"MASK\"],roi_size=(-1, -1, 1)),\n",
    "        # Resized(keys=[\"SRC\", \"TGT\"], spatial_size=[256,256,-1], mode=\"trilinear\"), # make it 256**2 to make sure we downsample correctly\n",
    "        SqueezeDimd(keys=[\"SRC\", \"TGT\", \"MASK\"], dim=0),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_dim_fixer_3d(target_nii,tgt_affine,src_affine):\n",
    "    newclip = target_nii\n",
    "    if tgt_affine[0] != src_affine[0]:\n",
    "        # check if they mirrored on X\n",
    "        newclip = np.flip(newclip,axis=0) \n",
    "    if tgt_affine[1] != src_affine[1]:\n",
    "        # check if they mirrored on Y\n",
    "        newclip = np.flip(newclip,axis=1) \n",
    "    return newclip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fee393-27c7-462c-9bde-6988375a3130",
   "metadata": {},
   "source": [
    "1) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731c3ca-172f-4bdc-86e4-58f6c9797105",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_DATA=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa819f-70e3-496d-a673-5bef40adeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    train_mr_outdir=os.path.join(MRs+suffix2d,'train_mr')\n",
    "    os.makedirs(train_mr_outdir, exist_ok=True)\n",
    "\n",
    "    train_ct_outdir=os.path.join(CTs + suffix2d,'train_ct')\n",
    "    os.makedirs(train_ct_outdir, exist_ok=True)\n",
    "\n",
    "    train_mask_outdir = os.path.join(Masks + suffix2d,'train_mask')\n",
    "    os.makedirs(train_mask_outdir, exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(len(train_dic_3d)),'Train'):       \n",
    "        transformed_image = train_transforms(train_dic_3d[i])\n",
    "\n",
    "        train_mr_fname_ref = transformed_image[0]['SRC_meta_dict']['filename_or_obj']\n",
    "        train_mr_nii_ref = nib.load(train_mr_fname_ref)\n",
    "\n",
    "        train_ct_fname_ref = transformed_image[0]['TGT_meta_dict']['filename_or_obj']\n",
    "        train_ct_nii_ref = nib.load(train_ct_fname_ref)\n",
    "        \n",
    "        train_mask_fname_ref = transformed_image[0]['MASK_meta_dict']['filename_or_obj']\n",
    "        train_mask_nii_ref = nib.load(train_mask_fname_ref)\n",
    "\n",
    "        for j in range(len(transformed_image)):\n",
    "            fname_out = os.path.join(train_mr_outdir, os.path.split(train_mr_fname_ref)[-1].split('.')[0]+('_sample%.3d.nii.gz' % j))\n",
    "            nib.save(nib.Nifti1Image(transformed_image[j]['SRC'][:,:,0].numpy(), None, train_mr_nii_ref.header), fname_out)\n",
    "\n",
    "            \n",
    "            fname_out = os.path.join(train_ct_outdir, os.path.split(train_ct_fname_ref)[-1].split('.')[0]+('_sample%.3d.nii.gz' % j))\n",
    "            fname_out_mask = os.path.join(train_mask_outdir, train_mask_fname_ref.split('/')[-2]+('_sample%.3d.nii.gz' % j))\n",
    "\n",
    "            try:\n",
    "                if train_ct_nii_ref.header.get_zooms()==train_mr_nii_ref.header.get_zooms():\n",
    "                    # flip the slice if they mirrored and save it with same spatial coords as reference one\n",
    "                    nib.save(nib.Nifti1Image(spatial_dim_fixer_3d(transformed_image[j]['TGT'][:,:,0].numpy(),train_ct_nii_ref.affine[:3,3],train_mr_nii_ref.affine[:3,3]), train_mr_nii_ref.affine,train_ct_nii_ref.header), fname_out)\n",
    "            except:\n",
    "                warnings.warn('The Niftis files don\"t contain same real-world coordinates (mm)')\n",
    "                nib.save(nib.Nifti1Image(transformed_image[j]['TGT'][:,:,0].numpy(), None,train_ct_nii_ref.header), fname_out)\n",
    "            try:\n",
    "                if train_mask_nii_ref.header.get_zooms()==train_mr_nii_ref.header.get_zooms():\n",
    "                    nib.save(nib.Nifti1Image(spatial_dim_fixer_3d(transformed_image[j]['MASK'][:,:,0].numpy(),train_mask_nii_ref.affine[:3,3],train_mr_nii_ref.affine[:3,3]), train_mr_nii_ref.affine,train_mask_nii_ref.header), fname_out_mask)\n",
    "            except:\n",
    "                warnings.warn('The Niftis files don\"t contain same real-world coordinates (mm)')\n",
    "                nib.save(nib.Nifti1Image(transformed_image[j]['MASK'][:,:,0].numpy(), None,train_mask_nii_ref.header), fname_out_mask)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9aee3-2d87-4054-850d-119bab7cc007",
   "metadata": {},
   "source": [
    "2) Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe52583-9373-4abd-aed5-117c859b2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_DATA:\n",
    "    val_mr_outdir=os.path.join(MRs + suffix2d,'val_mr')\n",
    "    os.makedirs(val_mr_outdir, exist_ok=True)\n",
    "    \n",
    "    val_ct_outdir=os.path.join(CTs + suffix2d,'val_ct')\n",
    "    os.makedirs(val_ct_outdir, exist_ok=True)\n",
    "\n",
    "    val_mask_outdir = os.path.join(Masks + suffix2d,'val_mask')\n",
    "    os.makedirs(val_mask_outdir, exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(len(val_dic_3d)),'Val'):\n",
    "        transformed_image = valtest_transforms(val_dic_3d[i])\n",
    "\n",
    "        fname_ref = transformed_image['SRC_meta_dict']['filename_or_obj']\n",
    "        nii_ref = nib.load(fname_ref)\n",
    "        mr_ref = nii_ref\n",
    "        fname_out = os.path.join(val_mr_outdir, os.path.split(fname_ref)[-1].split('.')[0]+('_sCenter.nii.gz'))\n",
    "        nib.save(nib.Nifti1Image(transformed_image['SRC'][:,:,0].numpy(), None, nii_ref.header), fname_out)\n",
    "\n",
    "        fname_ref = transformed_image['TGT_meta_dict']['filename_or_obj']\n",
    "        nii_ref = nib.load(fname_ref)\n",
    "        fname_out = os.path.join(val_ct_outdir, os.path.split(fname_ref)[-1].split('.')[0]+('_sCenter.nii.gz'))\n",
    "\n",
    "        fname_ref = transformed_image['MASK_meta_dict']['filename_or_obj']\n",
    "        nii_ref_mask = nib.load(fname_ref)\n",
    "        fname_out_mask = os.path.join(val_mask_outdir, fname_ref.split('/')[1]+('_sCenter.nii.gz'))\n",
    "        \n",
    "        try:\n",
    "            if nii_ref.header.get_zooms()==mr_ref.header.get_zooms():\n",
    "                nib.save(nib.Nifti1Image(spatial_dim_fixer_3d(transformed_image['TGT'][:,:,0].numpy(),nii_ref.affine[:3,3],mr_ref.affine[:3,3]), mr_ref.affine,nii_ref.header), fname_out)\n",
    "        except:\n",
    "            warnings.warn('The Niftis files don\"t contain same real-world coordinates (mm)')\n",
    "            nib.save(nib.Nifti1Image(transformed_image['TGT'][:,:,0].numpy(), None,nii_ref.header), fname_out)\n",
    "\n",
    "        try:\n",
    "            if nii_ref_mask.header.get_zooms()==mr_ref.header.get_zooms():\n",
    "                nib.save(nib.Nifti1Image(spatial_dim_fixer_3d(transformed_image['MASK'][:,:,0].numpy(),nii_ref_mask.affine[:3,3],mr_ref.affine[:3,3]), mr_ref.affine,nii_ref_mask.header), fname_out_mask)\n",
    "        except:\n",
    "            warnings.warn('The Niftis files don\"t contain same real-world coordinates (mm)')\n",
    "            nib.save(nib.Nifti1Image(transformed_image['MASK'][:,:,0].numpy(), None,nii_ref_mask.header), fname_out_mask) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398f7c8-036b-455b-9da9-f10a955870c9",
   "metadata": {},
   "source": [
    "# 2D processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a5777-b008-442a-8d0c-74a331f9535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_2d = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"SRC\", \"TGT\",\"MASK\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"SRC\", \"TGT\",\"MASK\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "BATCH_SIZE=1\n",
    "NUM_WORKERS=4\n",
    "\n",
    "MRs_sufx = os.path.join(MRs + suffix2d)\n",
    "CTs_sufx = os.path.join(CTs + suffix2d)\n",
    "Masks_sufix = os.path.join(Masks + suffix2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d2655-de9e-4134-a77b-c9be5bc044b0",
   "metadata": {},
   "source": [
    "- Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f482f-4d5c-46a5-aaa1-e32f320eb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_train_A_2d = sorted(glob.glob(os.path.join(MRs_sufx, 'train_mr', '*.nii.gz')))\n",
    "fnames_train_B_2d = sorted(glob.glob(os.path.join(CTs_sufx, 'train_ct', '*.nii.gz')))\n",
    "fnames_train_C_2d = sorted(glob.glob(os.path.join(Masks_sufix, 'train_ct', '*.nii.gz')))\n",
    "train_dic_2d = [{\"SRC\": img1, \"TGT\": img2, \"MASK\": img3} for (img1,img2,img3) in zip(\n",
    "    fnames_train_A_2d, \n",
    "    fnames_train_B_2d,\n",
    "    fnames_train_C_2d\n",
    ")] \n",
    "\n",
    "train_ds = SmartCacheDataset(train_dic_2d, transforms_2d,replace_rate=0.2,cache_rate=0.5)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a0c7b-baf1-48b3-bc24-49d6c907cac9",
   "metadata": {},
   "source": [
    "- Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3195a8-9fc3-4fc5-9268-60d7f025ded3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fnames_val_A_2d = sorted(glob.glob(os.path.join(MRs_sufx, 'val_mr', '*.nii.gz')))\n",
    "fnames_val_B_2d = sorted(glob.glob(os.path.join(CTs_sufx, 'val_ct', '*.nii.gz')))\n",
    "fnames_train_C_2d = sorted(glob.glob(os.path.join(Masks_sufix, 'val_mask', '*.nii.gz')))\n",
    "val_dic_2d = [{\"SRC\": img1, \"TGT\": img2, \"MASK\": img3} for (img1,img2,img3) in zip(\n",
    "    fnames_val_A_2d, \n",
    "    fnames_val_B_2d,\n",
    "    fnames_train_C_2d\n",
    ")] \n",
    "\n",
    "val_ds = SmartCacheDataset(val_dic_2d, transforms_2d,replace_rate=0.2,cache_rate=0.5)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6435c-fcbc-4666-87e0-63fdcb1648d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = first(val_loader)\n",
    "print(\"first patch's shape: \", check_data[\"SRC\"].shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(check_data[\"TGT\"][0,0,:,:].detach().cpu().numpy().squeeze(), vmin=-1, vmax=1, cmap=\"gray\")\n",
    "plt.title('Target')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(check_data[\"SRC\"][0,0,:,:].detach().cpu().numpy().squeeze(), vmin=-1, vmax=1, cmap=\"gray\")\n",
    "plt.title('Source')\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(check_data[\"MASK\"][0,0,:,:].detach().cpu().numpy().squeeze(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.title('mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540280b",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eee6ec-93e2-4943-9d26-29d52c9f3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vjnetworks import Pix2Pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a78f6-a65a-4ab4-9979-eaddb1412337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options():\n",
    "    def __init__(self):\n",
    "        # model parameters\n",
    "        self.in_channels = 1  # Adjust according to your input image channel dimensions\n",
    "        self.out_channels = 1  # Adjust according to your output image channel dimensions\n",
    "        self.num_filters_d = 128  # Adjust the number of filters in the discriminator\n",
    "        self.num_layers_d = 4  # Adjust the number of layers in the discriminator (i.e. the receptive field)\n",
    "        self.num_d = 2\n",
    "        self.num_res_units_G = 10\n",
    "        self.lambda_gan = 1 # Adjust the weight for the cycle consistency loss\n",
    "opt=Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851976a2-db22-4213-b76b-453c77513d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import monai.networks.nets as nets\n",
    "\n",
    "pix2pix_model = Pix2Pix(\n",
    "    in_channels=opt.in_channels,\n",
    "    out_channels=opt.out_channels,\n",
    "    num_d=opt.num_d,\n",
    "    num_layers_d=opt.num_layers_d,\n",
    "    num_filters_d=opt.num_filters_d,\n",
    "    num_res_units_G=opt.num_res_units_G,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f63cd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIDs_test = [file.split('_')[0] + '.nii.gz' for file in os.listdir(os.path.join('MR'+suffix2d,\"test_mr\"))]\n",
    "fnames_test_A = sorted([os.path.join(MRs,PID) for PID in PIDs_test])\n",
    "fnames_test_B = sorted([os.path.join(CTs,PID) for PID in PIDs_test])\n",
    "\n",
    "test_dic = [{\"SRC\": img1, \"TGT\": img2} for (img1,img2) in zip(\n",
    "    fnames_test_A, \n",
    "    fnames_test_B \n",
    ")] \n",
    "print(test_dic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, Transform, MapTransform\n",
    "from monai.transforms import ScaleIntensityRangePercentiles\n",
    "NUM_WORKERS=1\n",
    "BATCH_SIZE=1\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"SRC\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"SRC\"]),\n",
    "        ScaleIntensityRangePercentilesd(keys=[\"SRC\"], lower=1, upper=99.9, b_min=-1,b_max=1, clip=True),\n",
    "        EnsureTyped(keys=[\"SRC\"], dtype=torch.float32),\n",
    "        # Resized(keys=[\"SRC\"], spatial_size=[256,256,-1], mode=\"trilinear\"), # make it 256**2 to make sure we downsample correctly\n",
    "        # CenterSpatialCropd(keys=[\"SRC\"],roi_size=(256, 256, -1)),\n",
    "        # SqueezeDimd(keys=[\"SRC\", \"TGT\"], dim=3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from monai.transforms import Invertd\n",
    "test_post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "        keys='PRED', \n",
    "        transform=test_transforms, \n",
    "        orig_keys=\"SRC\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f299062",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset(test_dic, test_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=[116]\n",
    "ROI_SIZE = 256\n",
    "EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23448944",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_PREFIX=\"Gamma_crop-n2-l4-f128_GAN1_L140.00_NGF20.00_a0.08\"\n",
    "for EPOCH in EPOCHS:\n",
    "    weights_dir='Weights/'+EXPERIMENT_PREFIX\n",
    "    # weights_path = os.path.join(weights_dir, EXPERIMENT_PREFIX+'_e%.4d.h5' % EPOCH)\n",
    "    # weights_dir='Weights/'+\"Gamma_crop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10\"\n",
    "    # weights_path = os.path.join(weights_dir, \"Gamma_crop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10\"+'_e%.4d.h5' % EPOCH)\n",
    "    # checkpoint = torch.load(weights_path)\n",
    "    checkpoint = torch.load(\"Gamma_crop-n2-l4-f128_GAN1_L140.00_NGF20.00_a0.08_e0116.h5\")\n",
    "    pix2pix_model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model=pix2pix_model.generator_A_to_B\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    outdir='fakeCT_'+EXPERIMENT_PREFIX+'_e%.4d' % EPOCH\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "\n",
    "    roi_size = ((ROI_SIZE,ROI_SIZE))\n",
    "    sw_batch_size=2\n",
    "    slice_inferer = SliceInferer(\n",
    "        roi_size=roi_size,\n",
    "        sw_batch_size=sw_batch_size,\n",
    "        overlap=0.75,\n",
    "        mode='gaussian',\n",
    "        sigma_scale=0.5, # use if mode=\"gaussian\"\n",
    "        spatial_dim=2,  # axial inference\n",
    "        device=gpu_device,\n",
    "        padding_mode=\"replicate\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for i,test_data in enumerate(test_loader):\n",
    "            fname = test_dic[i]['SRC']\n",
    "            nii = nib.load(fname)\n",
    "            img = nii.get_fdata()\n",
    "            mask = img==0\n",
    "\n",
    "            test_images =test_data[\"SRC\"].to(gpu_device)\n",
    "\n",
    "            # '''\n",
    "            #256\n",
    "            # FAKE\n",
    "            test_data[\"PRED\"] = slice_inferer(test_images, model)\n",
    "            OUT  = [test_post_transforms(i) for i in decollate_batch(test_data)][0]\n",
    "            output = OUT['PRED'].detach().cpu().numpy().squeeze(axis=0)\n",
    "            output = ((output + 1) / 2) * 4000 - 1000\n",
    "            output[mask] = -1000\n",
    "            # '''\n",
    "            \n",
    "            # # 512\n",
    "            # # FAKE\n",
    "            # test_data[\"PRED\"] = sliding_window_inference(\n",
    "            #             roi_size=roi_size,\n",
    "            #             inputs=test_images,\n",
    "            #             sw_batch_size=sw_batch_size,\n",
    "            #             predictor=model,\n",
    "            #             overlap=0.75,\n",
    "            #             mode='gaussian',\n",
    "            #             sigma_scale=0.5,\n",
    "            #             device=gpu_device,\n",
    "            #             padding_mode=\"replicate\",\n",
    "            #         )\n",
    "            # output = test_data['PRED'].detach().cpu().numpy()[0, 0]\n",
    "            # output = ((output + 1) / 2) * 4000 - 1000\n",
    "            # output[mask] = -1000\n",
    "            \n",
    "\n",
    "            fname_out = os.path.join(outdir, fname.split('/')[-2].split('_')[-1]+'_'+fname.split('/')[-1].split('.')[0]+'_pTGT.nii.gz')\n",
    "            nii_out = nib.Nifti1Image(output.astype(np.int16), nii.affine, nii.header)\n",
    "            nib.save(nii_out, fname_out)\n",
    "            print(fname_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37405c12",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f2b75",
   "metadata": {},
   "source": [
    "## ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def dilate2d_volume(M, pixels=2):\n",
    "    # M: [B,1,D,H,W], dilate each axial slice with max-pool\n",
    "    k = 2*pixels+1\n",
    "    return F.max_pool3d(M, kernel_size=(1,k,k), stride=1, padding=(0,pixels,pixels))\n",
    "\n",
    "def _gauss1d(win, sigma, device):\n",
    "    ax = torch.arange(win, device=device, dtype=torch.float32) - (win - 1) / 2\n",
    "    g = torch.exp(-(ax**2) / (2 * sigma**2))\n",
    "    return g / g.sum()\n",
    "\n",
    "def _gauss3d_kernel(kw, kh, kd, sx, sy, sz, device):\n",
    "    gx = _gauss1d(kw, sx, device)\n",
    "    gy = _gauss1d(kh, sy, device)\n",
    "    gz = _gauss1d(kd, sz, device)\n",
    "    K = gz[:, None, None] * gy[None, :, None] * gx[None, None, :]\n",
    "    K = K / K.sum()\n",
    "    return K.view(1, 1, kd, kh, kw)\n",
    "\n",
    "def masked_ssim_3d(pred, gt, mask, data_range=2.0,\n",
    "                   win_size=(7,7,7), sigma=(1.5,1.5,1.5), eps=1e-8):\n",
    "    \"\"\"\n",
    "    pred, gt, mask: torch.Tensor [1,1,D,H,W] or [B,1,D,H,W], float32 in [-1,1], mask in {0,1}\n",
    "    Returns: scalar masked SSIM (float)\n",
    "    \"\"\"\n",
    "    assert pred.shape == gt.shape == mask.shape\n",
    "    if pred.ndim == 4:  # [1, D,H,W] -> [1,1,D,H,W]\n",
    "        pred = pred.unsqueeze(0)\n",
    "        gt   = gt.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "    assert pred.ndim == 5 and pred.shape[1] == 1\n",
    "\n",
    "    B, _, D, H, W = pred.shape\n",
    "    kd, kh, kw = win_size\n",
    "    sz, sy, sx = sigma\n",
    "    # ensure odd window and not exceeding dims\n",
    "    kd = max(3, min(kd, D));  kd = kd if kd % 2 == 1 else kd-1\n",
    "    kh = max(3, min(kh, H));  kh = kh if kh % 2 == 1 else kh-1\n",
    "    kw = max(3, min(kw, W));  kw = kw if kw % 2 == 1 else kw-1\n",
    "\n",
    "    K = _gauss3d_kernel(kw, kh, kd, sx, sy, sz, pred.device)\n",
    "    pad = (kw//2, kh//2, kd//2)\n",
    "\n",
    "    # windowed mask support\n",
    "    Wm = F.conv3d(mask, K, padding=pad) + eps  # [B,1,D,H,W]\n",
    "\n",
    "    # masked local means\n",
    "    mu_x = F.conv3d(pred*mask, K, padding=pad) / Wm\n",
    "    mu_y = F.conv3d(gt  *mask, K, padding=pad) / Wm\n",
    "\n",
    "    mu_x2, mu_y2 = mu_x**2, mu_y**2\n",
    "    sigma_x2 = F.conv3d((pred**2)*mask, K, padding=pad) / Wm - mu_x2\n",
    "    sigma_y2 = F.conv3d((gt**2)  *mask, K, padding=pad) / Wm - mu_y2\n",
    "    sigma_xy = F.conv3d((pred*gt)*mask, K, padding=pad) / Wm - mu_x*mu_y\n",
    "\n",
    "    C1 = (0.01 * data_range) ** 2\n",
    "    C2 = (0.03 * data_range) ** 2\n",
    "\n",
    "    num = (2*mu_x*mu_y + C1) * (2*sigma_xy + C2)\n",
    "    den = (mu_x2 + mu_y2 + C1) * (sigma_x2 + sigma_y2 + C2)\n",
    "    ssim_map = num / (den + eps)  # [B,1,D,H,W]\n",
    "\n",
    "    # weighted pooling by mask support (Wm), ignoring places with ~no mask in the window\n",
    "    ssim_perB = (ssim_map*Wm).flatten(1).sum(1) / Wm.flatten(1).sum(1).clamp_min(1.0)  # [B]\n",
    "    return float(ssim_perB.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_LOCATION = \"fakeCT_Gamma_nocrop-n2-l4-f128_GAN1_L140.00_NGF0.00_a0.08_V2_e0067\"\n",
    "# PRED_LOCATION = \"fakeCT_\" + EXPERIMENT_PREFIX + \"_e0041\"\n",
    "\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"PRED\", \"CT\", \"MASK\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"PRED\", \"CT\", \"MASK\"]),\n",
    "        ScaleIntensityRanged(keys=[\"PRED\", \"CT\"], a_min=-1000, a_max=3000, b_min=-1, b_max=1),\n",
    "        ResampleToMatchd(keys=[\"CT\",\"MASK\"], key_dst=\"PRED\"),\n",
    "        EnsureTyped(keys=[\"PRED\",\"CT\", \"MASK\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# PIDs_test = [file.split('_')[0] + '.nii.gz' for file in os.listdir(os.path.join('CT'+suffix2d,\"test_ct\"))]\n",
    "fnames_test_A = sorted(glob.glob(os.path.join(PRED_LOCATION,\"*.gz\")))\n",
    "fnames_test_B = sorted([os.path.join(CTs,PID) for PID in PIDs_test])\n",
    "fnames_test_D = sorted([glob.glob(os.path.join(\"CT_Seg_Mask\",PID.split('.')[0]+\"_mask\",\"*ext*\"))[0]for PID in PIDs_test])\n",
    "\n",
    "\n",
    "test_dic = [{\"PRED\": img1, \"CT\": img2, \"MASK\": img4} for (img1,img2,img4) in zip(\n",
    "    fnames_test_A, \n",
    "    fnames_test_B,\n",
    "    fnames_test_D,\n",
    ")] \n",
    "\n",
    "\n",
    "# test_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"PRED\", \"CT\"], image_only=False),\n",
    "#         EnsureChannelFirstd(keys=[\"PRED\", \"CT\"]),\n",
    "#         ScaleIntensityRanged(keys=[\"PRED\", \"CT\"], a_min=-1000, a_max=3000, b_min=-1, b_max=1),\n",
    "#         ResampleToMatchd(keys=[\"CT\"], key_dst=\"PRED\"),\n",
    "#         EnsureTyped(keys=[\"PRED\",\"CT\"]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# fnames_test_A = sorted(glob.glob(os.path.join(PRED_LOCATION,\"*.gz\")))\n",
    "# fnames_test_B = sorted([os.path.join(CTs,PID) for PID in PIDs_test])\n",
    "\n",
    "# test_dic = [{\"PRED\": img1, \"CT\": img2} for (img1,img2) in zip(\n",
    "#     fnames_test_A, \n",
    "#     fnames_test_B,\n",
    "# )] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ds = Dataset(test_dic[8:], test_transforms)\n",
    "\n",
    "# pred_dir = os.path.join(\"Preds\",PRED_LOCATION)\n",
    "# os.makedirs(pred_dir,exist_ok=True)\n",
    "\n",
    "# mean_ssim = []\n",
    "# gmean_ssim = []\n",
    "for sample in test_ds:\n",
    "    fprefix = sample['CT_meta_dict']['filename_or_obj'].split(\"/\")[-1].split('.')[0]\n",
    "\n",
    "    # fake = sample[\"PRED\"].to(gpu_device)\n",
    "    # center = fake.shape[-1] // 2\n",
    "    # ct_ = sample[\"CT\"].to(gpu_device)\n",
    "    # mr_ = sample[\"MR\"].to(gpu_device)\n",
    "    \n",
    "\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # plt.subplot(1,3,1)\n",
    "    # plt.title(\"MR\")\n",
    "    # plt.imshow(mr_[0,:,:,center].cpu().detach().numpy().squeeze(), vmin=-1, vmax=1, cmap='gray')\n",
    "    # plt.subplot(1,3,2)\n",
    "    # plt.title(\"CT\")\n",
    "    # # flipping the bozo ct\n",
    "    # plt.imshow(ct_[0,:,:,center].cpu().detach().numpy().squeeze(), vmin=-1, vmax=1, cmap='gray')\n",
    "    # plt.subplot(1,3,3)\n",
    "    # plt.title(\"Fake\")\n",
    "    # plt.imshow(fake[0,:,:,center].cpu().detach().numpy().squeeze(), vmin=-1, vmax=1, cmap='gray')\n",
    "    # fname_out = os.path.join(pred_dir, fprefix) \n",
    "    # plt.savefig(fname_out, bbox_inches='tight')\n",
    "    # plt.close() \n",
    "    \n",
    "    \n",
    "    #compute ssim \n",
    "    pred = sample[\"PRED\"].to(gpu_device).unsqueeze(0)\n",
    "    tgt = sample[\"CT\"].to(gpu_device).unsqueeze(0)\n",
    "    \n",
    "    # #global\n",
    "    # ssim_score,cs_score = compute_ssim_and_cs(pred, tgt,3,(11,11,11),(1.5,1.5,1.5),data_range=2)\n",
    "    # gmean_ssim.append(ssim_score.mean().item())\n",
    "    # print(fprefix, \"global\", round(ssim_score.mean().item(),6))\n",
    "    \n",
    "    # # local\n",
    "    mask_ = sample[\"MASK\"].to(gpu_device)\n",
    "    mask_ = dilate2d_volume(mask_, pixels=2)\n",
    "    mask_ = mask_.unsqueeze(0)\n",
    "    ssim_roi = masked_ssim_3d(pred, tgt, mask_, data_range=2.0,win_size=(11,11,11), sigma=(1.5,1.5,1.5))\n",
    "    # mean_ssim.append(ssim_roi)\n",
    "    print(fprefix, \"local\", round(ssim_roi,6))\n",
    "    \n",
    "\n",
    "# print(f\"Mean lSSIM : {torch.tensor(mean_ssim).mean():.6f}\")\n",
    "# print(f\"Mean gSSIM : {torch.tensor(gmean_ssim).mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c192e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### global ssim:\n",
    "# NGF 20 :: Mean SSIM : 0.726693\n",
    "# NGF 0 :: Mean SSIM : 0.639660 v2\n",
    "# best :: Mean SSIM : 0.560659\n",
    "\n",
    "### local ssim:\n",
    "# NGF 20 :: Mean SSIM : 0.513641\n",
    "# NGF 0 :: Mean SSIM : 0.320117 v2\n",
    "# best :: Mean SSIM : 0.505250\n",
    "\n",
    "\n",
    "###\n",
    "# EXP1 : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78358a24",
   "metadata": {},
   "source": [
    "## PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT_PREFIX=\"Gamma_Seg_nocrop-n2-l4-f128_GAN1_L140.00_NGF20.00_a0.08_fresh\"\n",
    "\n",
    "weights_dir='Weights/'+EXPERIMENT_PREFIX\n",
    "weights_path = os.path.join(weights_dir, EXPERIMENT_PREFIX+'_e%.4d.h5' % 41)\n",
    "# weights_dir='Weights/'+\"Gamma_crop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10\"\n",
    "# weights_path = os.path.join(weights_dir, \"Gamma_crop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10\"+'_e%.4d.h5' % EPOCH)\n",
    "checkpoint = torch.load(weights_path)\n",
    "# checkpoint = torch.load(\"Weights/Gamma_crop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10/Gamma_crop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10_e0070.h5\")\n",
    "pix2pix_model.load_state_dict(checkpoint['model'], strict=False)\n",
    "model=pix2pix_model.generator_A_to_B\n",
    "model.eval()\n",
    "\n",
    "# PRED_LOCATION = \"fakeCT_Gamma_nocrop-n2-l4-f128_GAN1_L100.00_NGF20.00_a0.10_e0070\"\n",
    "PRED_LOCATION = \"fakeCT_\" + EXPERIMENT_PREFIX + \"_e0041\"\n",
    "\n",
    "NUM_WORKERS=1\n",
    "BATCH_SIZE=1\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"PRED\", \"CT\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"PRED\", \"CT\"]),\n",
    "        ScaleIntensityRanged(keys=[\"PRED\", \"CT\"], a_min=-1000, a_max=3000, b_min=-1, b_max=1),\n",
    "        EnsureTyped(keys=[\"PRED\",\"CT\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# PIDs_test = [file.split('_')[0] + '.nii.gz' for file in os.listdir(os.path.join('CT'+suffix2d,\"test_ct\"))]\n",
    "fnames_test_A = sorted(glob.glob(os.path.join(PRED_LOCATION,\"*.gz\")))\n",
    "fnames_test_B = sorted([os.path.join(CTs,PID) for PID in PIDs_test])\n",
    "fnames_test_C = sorted([os.path.join(MRs,PID) for PID in PIDs_test])\n",
    "\n",
    "\n",
    "test_dic = [{\"PRED\": img1, \"CT\": img2} for (img1,img2) in zip(\n",
    "    fnames_test_A, \n",
    "    fnames_test_B\n",
    ")] \n",
    "\n",
    "\n",
    "test_ds = Dataset(test_dic, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0744cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.exposure import match_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368104cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_psnr = []\n",
    "for sample in test_ds:\n",
    "\n",
    "    fake = sample[\"PRED\"].to(gpu_device)\n",
    "    ct_ = sample[\"CT\"].to(gpu_device)\n",
    "\n",
    "    fake_np = fake.squeeze().cpu().numpy().astype(np.float32)\n",
    "    ct_np   = ct_.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    fake_np = np.clip(fake_np, -1.0, 1.0)\n",
    "    ct_np   = np.clip(ct_np,   -1.0, 1.0)\n",
    "\n",
    "    c_psnr = psnr(ct_np, fake_np, data_range=2)\n",
    "    mean_psnr.append(c_psnr)\n",
    "    print(f\"{sample['CT_meta_dict']['filename_or_obj'].split('/')[-1].split('.')[0]}'s psnr: {c_psnr}\")\n",
    "\n",
    "print(f\"Mean PSNR : {torch.tensor(mean_psnr).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a23ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = decollate_batch(test_data)\n",
    "OUT_processed = (OUT)[0]['PRED']\n",
    "OUT_processed_np = OUT_processed.detach().cpu().numpy().squeeze()\n",
    "OUT_processed_np.shape\n",
    "plt.imshow(OUT_processed_np[:,:,85])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba3eb9",
   "metadata": {},
   "source": [
    "## Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec00980",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_LOCATION = \"CT_NGF0_V2_skull\"\n",
    "# PRED_LOCATION = \"fakeCT_\" + EXPERIMENT_PREFIX + \"_e0041\"\n",
    "\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"PRED\", \"CT\"], image_only=False),\n",
    "        EnsureChannelFirstd(keys=[\"PRED\", \"CT\"]),\n",
    "        EnsureTyped(keys=[\"PRED\",\"CT\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "PIDs_test = [file.split('_')[1] for file in os.listdir(\"fakeCT_Gamma_crop-n2-l4-f128_GAN1_L140.00_NGF20.00_a0.08_e0041\")]\n",
    "fnames_test_A = sorted(glob.glob(os.path.join(PRED_LOCATION,\"*\",\"*gz\")))\n",
    "fnames_test_B = sorted([os.path.join(CTs,PID+\".nii.gz\") for PID in PIDs_test])\n",
    "\n",
    "test_dic = [{\"PRED\": img1, \"CT\": img2} for (img1,img2) in zip(\n",
    "    fnames_test_A, \n",
    "    fnames_test_B\n",
    ")] \n",
    "\n",
    "\n",
    "test_ds = Dataset(test_dic, test_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfade134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric, MeanIoU\n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n",
    "\n",
    "mean_dice = []\n",
    "for sample in test_ds:\n",
    "    pred = sample[\"PRED\"].to(gpu_device).unsqueeze(0).float()\n",
    "    gt   = sample[\"CT\"].to(gpu_device).unsqueeze(0).float()\n",
    "    \n",
    "    d = dice_metric(y_pred=pred, y=gt) \n",
    "\n",
    "    mean_dice.append(d.item())\n",
    "\n",
    "    pid = sample['CT_meta_dict']['filename_or_obj'].split('/')[-1].split('.')[0]\n",
    "    print(f\"{pid} â€” Dice: {d.item():.4f}\")\n",
    "\n",
    "print(f\"\\nMean Dice: {sum(mean_dice)/len(mean_dice):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
